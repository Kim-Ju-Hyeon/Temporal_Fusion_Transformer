{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "549837b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b30bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2744d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "459e81f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caa725fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "\n",
    "data = get_stallion_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2f43557",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5397e5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "\n",
    "# add additional features\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")  # categories have be strings\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = data.groupby([\"time_idx\", \"sku\"], observed=True).volume.transform(\"mean\")\n",
    "data[\"avg_volume_by_agency\"] = data.groupby([\"time_idx\", \"agency\"], observed=True).volume.transform(\"mean\")\n",
    "\n",
    "# we want to encode special days as one variable and thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\",\n",
    "    \"good_friday\",\n",
    "    \"new_year\",\n",
    "    \"christmas\",\n",
    "    \"labor_day\",\n",
    "    \"independence_day\",\n",
    "    \"revolution_day_memorial\",\n",
    "    \"regional_games\",\n",
    "    \"fifa_u_17_world_cup\",\n",
    "    \"football_gold_cup\",\n",
    "    \"beer_capital\",\n",
    "    \"music_fest\",\n",
    "]\n",
    "data[special_days] = data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03e47953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>volume</th>\n",
       "      <th>industry_volume</th>\n",
       "      <th>soda_volume</th>\n",
       "      <th>avg_max_temp</th>\n",
       "      <th>price_regular</th>\n",
       "      <th>price_actual</th>\n",
       "      <th>discount</th>\n",
       "      <th>avg_population_2017</th>\n",
       "      <th>avg_yearly_household_income_2017</th>\n",
       "      <th>discount_in_percent</th>\n",
       "      <th>timeseries</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>log_volume</th>\n",
       "      <th>avg_volume_by_sku</th>\n",
       "      <th>avg_volume_by_agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>2.100000e+04</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.00000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "      <td>21000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1492.403982</td>\n",
       "      <td>5.439214e+08</td>\n",
       "      <td>8.512000e+08</td>\n",
       "      <td>28.612404</td>\n",
       "      <td>1451.536344</td>\n",
       "      <td>1267.347450</td>\n",
       "      <td>184.374146</td>\n",
       "      <td>1.045065e+06</td>\n",
       "      <td>151073.494286</td>\n",
       "      <td>10.574884</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>2.464118</td>\n",
       "      <td>1492.403982</td>\n",
       "      <td>1492.403982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2711.496882</td>\n",
       "      <td>6.288022e+07</td>\n",
       "      <td>7.824340e+07</td>\n",
       "      <td>3.972833</td>\n",
       "      <td>683.362417</td>\n",
       "      <td>587.757323</td>\n",
       "      <td>257.469968</td>\n",
       "      <td>9.291926e+05</td>\n",
       "      <td>50409.593114</td>\n",
       "      <td>9.590813</td>\n",
       "      <td>101.03829</td>\n",
       "      <td>17.318515</td>\n",
       "      <td>8.178218</td>\n",
       "      <td>1051.790829</td>\n",
       "      <td>1328.239698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.130518e+08</td>\n",
       "      <td>6.964015e+08</td>\n",
       "      <td>16.731034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3121.690141</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.227100e+04</td>\n",
       "      <td>90240.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-18.420681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.272388</td>\n",
       "      <td>5.090553e+08</td>\n",
       "      <td>7.890880e+08</td>\n",
       "      <td>25.374816</td>\n",
       "      <td>1311.547158</td>\n",
       "      <td>1178.365653</td>\n",
       "      <td>54.935108</td>\n",
       "      <td>6.018900e+04</td>\n",
       "      <td>110057.000000</td>\n",
       "      <td>3.749628</td>\n",
       "      <td>87.00000</td>\n",
       "      <td>14.750000</td>\n",
       "      <td>2.112923</td>\n",
       "      <td>932.285496</td>\n",
       "      <td>113.420250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>158.436000</td>\n",
       "      <td>5.512000e+08</td>\n",
       "      <td>8.649196e+08</td>\n",
       "      <td>28.479272</td>\n",
       "      <td>1495.174592</td>\n",
       "      <td>1324.695705</td>\n",
       "      <td>138.307225</td>\n",
       "      <td>1.232242e+06</td>\n",
       "      <td>131411.000000</td>\n",
       "      <td>8.948990</td>\n",
       "      <td>174.50000</td>\n",
       "      <td>29.500000</td>\n",
       "      <td>5.065351</td>\n",
       "      <td>1402.305264</td>\n",
       "      <td>1730.529771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1774.793475</td>\n",
       "      <td>5.893715e+08</td>\n",
       "      <td>9.005551e+08</td>\n",
       "      <td>31.568405</td>\n",
       "      <td>1725.652080</td>\n",
       "      <td>1517.311427</td>\n",
       "      <td>272.298630</td>\n",
       "      <td>1.729177e+06</td>\n",
       "      <td>206553.000000</td>\n",
       "      <td>15.647058</td>\n",
       "      <td>262.00000</td>\n",
       "      <td>44.250000</td>\n",
       "      <td>7.481439</td>\n",
       "      <td>2195.362302</td>\n",
       "      <td>2595.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22526.610000</td>\n",
       "      <td>6.700157e+08</td>\n",
       "      <td>1.049869e+09</td>\n",
       "      <td>45.290476</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>4925.404000</td>\n",
       "      <td>19166.625000</td>\n",
       "      <td>3.137874e+06</td>\n",
       "      <td>247220.000000</td>\n",
       "      <td>226.740147</td>\n",
       "      <td>349.00000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>10.022453</td>\n",
       "      <td>4332.363750</td>\n",
       "      <td>5884.717375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             volume  industry_volume   soda_volume  avg_max_temp  \\\n",
       "count  21000.000000     2.100000e+04  2.100000e+04  21000.000000   \n",
       "mean    1492.403982     5.439214e+08  8.512000e+08     28.612404   \n",
       "std     2711.496882     6.288022e+07  7.824340e+07      3.972833   \n",
       "min        0.000000     4.130518e+08  6.964015e+08     16.731034   \n",
       "25%        8.272388     5.090553e+08  7.890880e+08     25.374816   \n",
       "50%      158.436000     5.512000e+08  8.649196e+08     28.479272   \n",
       "75%     1774.793475     5.893715e+08  9.005551e+08     31.568405   \n",
       "max    22526.610000     6.700157e+08  1.049869e+09     45.290476   \n",
       "\n",
       "       price_regular  price_actual      discount  avg_population_2017  \\\n",
       "count   21000.000000  21000.000000  21000.000000         2.100000e+04   \n",
       "mean     1451.536344   1267.347450    184.374146         1.045065e+06   \n",
       "std       683.362417    587.757323    257.469968         9.291926e+05   \n",
       "min         0.000000  -3121.690141      0.000000         1.227100e+04   \n",
       "25%      1311.547158   1178.365653     54.935108         6.018900e+04   \n",
       "50%      1495.174592   1324.695705    138.307225         1.232242e+06   \n",
       "75%      1725.652080   1517.311427    272.298630         1.729177e+06   \n",
       "max     19166.625000   4925.404000  19166.625000         3.137874e+06   \n",
       "\n",
       "       avg_yearly_household_income_2017  discount_in_percent   timeseries  \\\n",
       "count                      21000.000000         21000.000000  21000.00000   \n",
       "mean                      151073.494286            10.574884    174.50000   \n",
       "std                        50409.593114             9.590813    101.03829   \n",
       "min                        90240.000000             0.000000      0.00000   \n",
       "25%                       110057.000000             3.749628     87.00000   \n",
       "50%                       131411.000000             8.948990    174.50000   \n",
       "75%                       206553.000000            15.647058    262.00000   \n",
       "max                       247220.000000           226.740147    349.00000   \n",
       "\n",
       "           time_idx    log_volume  avg_volume_by_sku  avg_volume_by_agency  \n",
       "count  21000.000000  21000.000000       21000.000000          21000.000000  \n",
       "mean      29.500000      2.464118        1492.403982           1492.403982  \n",
       "std       17.318515      8.178218        1051.790829           1328.239698  \n",
       "min        0.000000    -18.420681           0.000000              0.000000  \n",
       "25%       14.750000      2.112923         932.285496            113.420250  \n",
       "50%       29.500000      5.065351        1402.305264           1730.529771  \n",
       "75%       44.250000      7.481439        2195.362302           2595.316500  \n",
       "max       59.000000     10.022453        4332.363750           5884.717375  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7eb15f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 6\n",
    "max_encoder_length = 24\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # keep encoder length long (as it is in the validation set)\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "    time_varying_known_reals=[\"time_idx\", \"price_regular\", \"discount_in_percent\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], transformation=\"softplus\"\n",
    "    ),  # use softplus and normalize by group\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    ")\n",
    "\n",
    "# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "# for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data, predict=True, stop_randomization=True)\n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 1  # set this between 32 to 128\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be95967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agency                                        Agency_22\n",
       "sku                                              SKU_01\n",
       "volume                                           52.272\n",
       "date                                2013-01-01 00:00:00\n",
       "industry_volume                               492612703\n",
       "soda_volume                                   718394219\n",
       "avg_max_temp                                  25.845238\n",
       "price_regular                               1168.903668\n",
       "price_actual                                1069.166193\n",
       "discount                                      99.737475\n",
       "avg_population_2017                               48151\n",
       "avg_yearly_household_income_2017                 132110\n",
       "easter_day                                            -\n",
       "good_friday                                           -\n",
       "new_year                                       new_year\n",
       "christmas                                             -\n",
       "labor_day                                             -\n",
       "independence_day                                      -\n",
       "revolution_day_memorial                               -\n",
       "regional_games                                        -\n",
       "fifa_u_17_world_cup                                   -\n",
       "football_gold_cup                                     -\n",
       "beer_capital                                          -\n",
       "music_fest                                            -\n",
       "discount_in_percent                            8.532566\n",
       "timeseries                                            0\n",
       "time_idx                                              0\n",
       "month                                                 1\n",
       "log_volume                                     3.956461\n",
       "avg_volume_by_sku                           2613.377501\n",
       "avg_volume_by_agency                          103.80546\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4273c416",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'encoder_cat': tensor([[[ 0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4],\n",
       "           [ 0,  0,  3,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  5],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6],\n",
       "           [ 0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  7],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  9],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0, 11],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,  0,  0,  0,  2],\n",
       "           [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  3],\n",
       "           [ 0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  4],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  5],\n",
       "           [ 0,  0,  3,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  6],\n",
       "           [ 0,  0,  0,  0,  0,  0,  6,  0,  0,  0,  0,  0,  0,  0,  7],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  8],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  9],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  5,  0,  0,  0,  0,  0,  0, 11],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  1],\n",
       "           [ 0,  0,  0,  0,  0,  0,  0,  0, 10,  9,  0,  0,  0,  0,  2],\n",
       "           [ 0,  0,  0,  0,  0,  2,  0,  0,  0,  0,  0,  0,  0,  0,  3]]]),\n",
       "  'encoder_cont': tensor([[[-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.7003, -0.3899,\n",
       "            -0.0345, -1.0000, -0.1436,  0.2459, -0.7130, -1.7344, -2.9171,\n",
       "            -1.0676,  1.0738],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.6361, -0.3899,\n",
       "            -0.3378, -0.9583,  0.2956,  0.2693, -1.7246, -1.2169, -2.1644,\n",
       "            -1.0561,  1.3626],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.5719, -0.3338,\n",
       "            -0.3414, -0.9167,  1.1960,  0.3063, -0.4351,  0.7957, -0.9712,\n",
       "            -1.0254,  1.6461],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.5078, -0.2636,\n",
       "            -0.2792, -0.8750,  1.5398,  0.3179, -0.0498,  0.0083, -0.2134,\n",
       "            -1.0256,  1.9322],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.4436, -0.2582,\n",
       "            -0.3435, -0.8333,  2.2546,  0.3389,  0.2731,  0.3914,  0.2684,\n",
       "            -1.0038,  2.0960],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.3795, -0.2516,\n",
       "            -0.2656, -0.7917,  2.3856,  0.3424, -0.0616,  0.7569,  1.3437,\n",
       "            -0.9988,  1.6413],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.3153, -0.2515,\n",
       "            -0.3845, -0.7500,  1.6025,  0.3199, -0.4502,  0.4693,  0.2622,\n",
       "            -1.0219,  1.5581],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.2511, -0.2516,\n",
       "            -0.6378, -0.7083,  1.8890,  0.3286, -0.0483, -1.1590,  0.1919,\n",
       "            -1.0236,  1.6390],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.1870, -0.2516,\n",
       "            -0.3348, -0.6667,  1.5507,  0.3182, -1.3525, -0.6974, -0.3799,\n",
       "            -1.0353,  1.0854],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.1228, -0.2511,\n",
       "            -0.2511, -0.6250,  1.5916,  0.3196, -0.9117, -0.6678, -0.5722,\n",
       "            -1.0275,  1.4858],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -1.0587, -0.2510,\n",
       "            -0.1698, -0.5833,  0.2274,  0.2659, -1.6742, -0.4553, -2.3004,\n",
       "            -1.0614,  1.3193],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.9945, -0.2510,\n",
       "             0.4806, -0.5417,  0.4593,  0.2769, -1.8546, -0.7573, -2.8006,\n",
       "            -1.0549,  1.9437],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.9303, -0.2510,\n",
       "             1.0457, -0.5000,  0.3584,  0.2722, -0.9005, -1.0745, -2.3949,\n",
       "            -1.0634,  1.0750],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.8662, -0.2510,\n",
       "             0.0216, -0.4583,  0.9395,  0.2969, -1.8314, -1.5007, -1.3023,\n",
       "            -1.0580,  1.6165],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.8020, -0.2510,\n",
       "             0.0532, -0.4167,  0.9805,  0.2984,  0.3633, -0.1347, -1.3460,\n",
       "            -1.0534,  1.9782],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.7379, -0.2510,\n",
       "             0.0464, -0.3750, -0.4001,  0.2298, -0.3506,  0.5270, -0.4622,\n",
       "            -1.0408,  2.3914],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.6737, -0.2079,\n",
       "            -0.9987, -0.3333, -1.6279,  0.0902,  0.6554,  0.7270,  0.2774,\n",
       "            -1.0349,  1.5822],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.6095, -0.1950,\n",
       "            -0.9961, -0.2917, -0.4956,  0.2233,  0.3078,  0.4021,  1.3014,\n",
       "            -1.0416,  1.8276],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.5454, -0.1913,\n",
       "            -1.0254, -0.2500,  0.0692,  0.2578,  0.2545, -0.0040,  0.4325,\n",
       "            -1.0466,  1.6988],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.4812, -0.1763,\n",
       "            -0.6714, -0.2083,  0.5303,  0.2801, -0.2109,  0.4347,  0.3567,\n",
       "            -1.0589,  1.8726],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.4170, -0.1496,\n",
       "            -0.1886, -0.1667,  0.3666,  0.2726, -2.0395, -1.0212, -0.0364,\n",
       "            -1.0683,  1.6711],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.3529, -0.1512,\n",
       "            -0.6081, -0.1250,  0.6803,  0.2865, -1.6096, -0.6793, -0.0696,\n",
       "            -1.0668,  1.6847],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.2887, -0.1526,\n",
       "            -0.5609, -0.0833, -0.4601,  0.2258, -0.8544, -1.7724, -2.0370,\n",
       "            -1.0781,  1.4106],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.2246, -0.1525,\n",
       "            -0.4193, -0.0417, -0.2746,  0.2380, -0.2135, -2.0545, -2.2905,\n",
       "            -1.0745,  2.0817]]]),\n",
       "  'encoder_target': tensor([[ 80.6760,  98.0640, 133.7040, 147.3120, 175.6080, 180.7920, 149.7960,\n",
       "           161.1360, 147.7440, 149.3640,  95.3640, 104.5440, 100.5480, 123.5520,\n",
       "           125.1720,  70.5240,  21.9240,  66.7440,  89.1000, 107.3520, 100.8720,\n",
       "           113.2920,  68.1480,  75.4920]]),\n",
       "  'encoder_lengths': tensor([24]),\n",
       "  'decoder_cat': tensor([[[0, 0, 0, 0, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5],\n",
       "           [0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6],\n",
       "           [0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 0, 7],\n",
       "           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8]]]),\n",
       "  'decoder_cont': tensor([[[-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.1604, -0.1538,\n",
       "            -0.5659,  0.0000, -1.1313,  0.1667, -0.6137, -1.0963, -3.0043,\n",
       "            -1.0906,  1.1821],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.0962, -0.1518,\n",
       "            -0.5462,  0.0417, -0.7084,  0.2071, -1.9578, -1.7539, -1.9492,\n",
       "            -1.0912,  1.4105],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598, -0.0321, -0.1511,\n",
       "            -0.0537,  0.0833,  0.0255,  0.2555, -0.3319,  0.4902, -2.0794,\n",
       "            -1.0806,  2.7088],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598,  0.0321, -0.0832,\n",
       "             0.1176,  0.1250, -0.1355,  0.2464, -0.5369,  0.3310, -0.9971,\n",
       "            -1.0720,  2.2298],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598,  0.0962, -0.0568,\n",
       "             0.2383,  0.1667,  0.1401,  0.2615,  0.6155,  1.3146, -0.0559,\n",
       "            -1.0724,  2.1258],\n",
       "           [-0.9593, -0.6123,  1.0000, -0.5347, -0.5598,  0.1604,  0.0204,\n",
       "             0.1843,  0.2083,  0.0064,  0.2544,  0.2432,  0.8992,  0.7383,\n",
       "            -1.0621,  1.8439]]]),\n",
       "  'decoder_target': tensor([[41.5800, 58.3200, 87.3720, 81.0000, 91.9080, 86.6160]]),\n",
       "  'decoder_lengths': tensor([6]),\n",
       "  'decoder_time_idx': tensor([[24, 25, 26, 27, 28, 29]]),\n",
       "  'groups': tensor([[0, 0]]),\n",
       "  'target_scale': tensor([[87.3620, 39.5837]])},\n",
       " (tensor([[41.5800, 58.3200, 87.3720, 81.0000, 91.9080, 86.6160]]), None))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40512279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "(actuals - baseline_predictions).abs().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c8c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf37a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a88010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,\n",
    "    weights_summary=\"top\",\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f45e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
